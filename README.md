Stock Movement Prediction with CNN-ViT on Candlestick ChartsThis project explores the use of a hybrid Convolutional Neural Network (CNN) and Vision Transformer (ViT) model to analyze and predict stock price movements. The core methodology treats financial prediction as a computer vision problem by training the model on images of candlestick charts rather than raw time-series data.The repository contains two primary experiments:Regression Model: An initial attempt to predict the precise 5-day future log-return of a stock.Classification Model: A pivot to a binary classification task to predict whether a stock's price will move up or down over the 5-day forecast horizon.ðŸ“Š MethodologyThe entire workflow is built on the idea of converting 1-dimensional time-series data into 2-dimensional image data to leverage powerful computer vision architectures.Data Collection:OHLCV (Open, High, Low, Close, Volume) data for four major tech stocks (META, AAPL, MSFT, GOOGL) was downloaded using yfinance.The data spans from January 1, 2015, to October 1, 2025, and is saved in market_data.csv.Image Generation:A sliding window approach is used to create samples. Each sample consists of a 120-day lookback window.Using mplfinance, each 120-day window is rendered as a 400x600px candlestick chart image (including volume bars).These images are cached as .npz files to the chart_cache/ (for regression) and chart_cache_class/ (for classification) directories to speed up training.Target Generation:For Regression: The target is the 5-day future log-return, calculated as $log(\frac{Price_{t+5}}{Price_t})$.For Classification: The target is a binary class (0 or 1), representing whether the 5-day future log-return was negative or positive.ðŸ§  Model Architecture: CNN-ViT HybridBoth experiments use the same hybrid model architecture, which combines the strengths of CNNs and Transformers:CNN Backbone (ConvNet): A custom CNN acts as a powerful feature extractor. It processes the raw candlestick chart image through a series of Convolution, BatchNorm, and MaxPool layers to generate high-level feature maps.Patch Embedding (PatchEmbed): A 1x1 Convolution layer flattens the CNN's feature maps into a 1D sequence of patches (tokens), making it suitable for a Transformer.Vision Transformer (ViT):A special [CLS] token is prepended to the sequence of patches.Positional embeddings are added to retain spatial information.The sequence is processed by a stack of TransformerBlock modules, which use Multi-Head Self-Attention (MHSA) to learn relationships between different parts of the chart.The final output state of the [CLS] token is passed to a linear head for the final prediction.ðŸ“ˆ Experiments & ResultsExperiment 1: Regression (Price Prediction)File: data.ipynbGoal: Predict the exact 5-day future log-return.Loss Function: Trained with both MSE and Huber Loss.Results: This task proved to be extremely difficult. The best model (using Huber Loss) achieved a test Root Mean Square Error (RMSE) of ~0.0331, or a 3.3% log-return error. The RÂ² score was negative, indicating the model performed worse than a simple baseline (e.g., predicting the mean return). This highlights the inefficiency of trying to predict exact future prices from charts alone.Experiment 2: Classification (Directional Prediction)File: classification_model.ipynbGoal: Pivot to a simpler, more realistic binary classification task (will the price be higher or lower in 5 days?).Data Handling:A per-ticker, sequential train/validation split (80/20) was used to prevent data leakage.Undersampling was applied to both the training and validation sets to create a balanced 50/50 class distribution.Loss Function: FocalLoss was used to help the model focus on harder-to-classify examples.Results: This approach yielded more promising results. The model achieved a peak validation accuracy of ~54.1%. While this is a small edge, a consistent accuracy above 50% is significant in financial markets. The final confusion matrix confirms that the model performs slightly better than a random 50/50 guess on both "up" and "down" classes.

